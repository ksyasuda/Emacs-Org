#+TITLE: Ch2 Intelligent Agents

* agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators
** robotic agent might have cameras and infrared range finders for sensors and various motors for actuators
* percept refers to the agent's perceptual inputs at any given instant
* percept sequence is the complete history of everytyhing the agent has ever perceived
** agent's choice of action at any given instance can depend on the entire percept sequence observed to date, but not on anything it hasn't perceived
* agen'ts behavior is described by the agent function
* rational agent: one that does the right thing, conceptually speaking, every entry in the table for the agent function is filled out correctly
* notion of desirability is captured by a performance measure
* rationality
** performance measure that defines the criterion of success
** agent's prior knowledge of the env
** actions that the agent can perform
** agent's percept sequence to date
* rational agents will perform as good or better when compared to any other agent
* omniscience
** omnicient agent knows the outcome of its actions and can act accordingly
* task environment
** P: Performance
*** safe, fast, legal, comfortable trip
** E: Environment
*** roads, dirt, traffic
** A: Actuators
*** steering accelerator, brake, signals
** S: Sensors
*** cameras, sonar, speedometer, GPS, odometer, engine sensors
** actuators for automated taxi include
*** human driver; control over the engine through accelerator and control over seetring and breaking
** sensors include
*** video camera, sonar, speedometer, accelerometer
* software agents
** softbots, exists in rich, unlimited domains
* properties of task env
** fully observable:
*** agen'ts sensors give it access to the complete state of the environment at each point in time
** partially observable
*** partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data, for example, a vacuum agent with only a local dirt sensor cannot tell whether there is dirt in other squares, and an automated taxi cannot see what other drivers are thinking
** if agent has no sensors at all then the environment is unobservable
** single agent vs multiagent
*** solving a crossword puzzle = single agent
*** chess is a competitive multiagent environment
** deterministic if the next state of the environment is completely determined by the current state and the action executed by the agent
** stochastic otherwise
** an environment is uncertain if it is not fully observable or not deterministic
** nondeterministic env is one in whcih actions are characterized by their possible outcomes, but no probabilities are attached to them
** episodic vs sequential
*** episodic task env, the agent's experience is divided into atomic episodes.  in each episode, the agent receives a percept and then performs a single action
**** the next episode does not depend on the actions taken in previous episodes
*** many classification tasks are episodic
**** agent that has to spot defetive parts on an assembly line bases each decision on the current part, regardless of previous decisions; moreover, the current decision doesn't affect whether the next part is defective
*** sequential env have current decision that could affect the future decision
**** chess and taxi driving are sequential
**** in both cases, short-term actions can have long-term consequences
** static vs dynamic
*** if the env can change while an agent is deliberating, then we say the env is dynamic for that agent
*** otherwise it is static
** if the env itself does not change with the passage of time but the agent's performance score does, then we say the env is semidynamic
*** taxi driving is clearly dynamic, chess when played with a clock is semidynamic.  crossword puzzles are static
** discrete vs continuous
*** applies to the state of the env, the way time is handled, and to the percepts and actions of the agent
*** chess env has a finite number of distinct states
**** also has a discrete set of percepts and actions
*** taxi driving is continuous-state and continuous-time problem
**** the speed and location of the taxi and of the other vehicles sweep through a range of continuous values and do so smoothly over time
**** the actions for driving also continuous
***** steering, angles, etc
**** input from digital camera is discrete, strictly speaking, but is typically treated as representing continuously varying intensities and locations
** known vs unknown
*** distinction refers not to the env itself, but to the agent's state of knowledge about the "laws of physics" of the env
*** in a known env, the outcomes for all actions are given
*** if env is unknown, the agent will have to learn how it works in order to make good decisions
* agent = architecture + program
* simple reflex agent
** select actions on the basis of the current percept, ignoring the rest of the percept history
** vacuum agent is simple reflex agent, because its decison is based only on the current location and on whether that location contains dirt
** condition-action rule
*** if car in front is braking; then initiate own breaking
** escape from infinite loops is possible if the agent can randomize its actions
* model based reflex agents
** best way to handle partial observability is for agent to keep track of the part of the world it can't see now
*** agent should maintain some soft of internal state that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state
*** braking problem:
**** internal state is not too extensive just the previous frame from the camera to check if brake lights are on
*** need some info about how the world evolves independently of the agent
*** need info about how the agent's own actions afffect the world
*** knowledge about how the world works is called a model of the world
** hard to determine the current state of a partially ovservable env exactly
*** agent makes its best guess
* goal based agents
** knowing something about the current state of the env is not always enough to decide what to do
** as well as state, agent needs some sort of goal information that describes situations that are desirable
** more flexible because the knowledge that supports its decisions is represented explicitly and can be modified
** agent's behavior can be changed easily to go to a different desitnation by specifying the destination as the goal
* utility based agents
** goals just provide a curde binary distinction between happy and unhappy
** more general performance measure is utility
** utility function is essentially an internalization of the performance measure
** solves the problem of multiple goals that can or cannot all be completed
*** complete the thing that brings the most utility
** rational utility-based agents choose the action that maximizes the expected utility of the action outcomes
*** expects to derive, on average, given the probabilities and utilities of each outcome
